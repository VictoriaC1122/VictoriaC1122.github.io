<!DOCTYPE html>
<html lang="en-US">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-N4LXQ0S6MC"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-N4LXQ0S6MC");
    </script>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Chordal Graph and Semidefinite Programs</title>
    <meta
      name="description"
      lang="en"
      content="This post is about chordal graph and semidefinite programs (SDP), one of the research topic I found very interesting. The purpose of this post is to help people understand the basic concept of chordal graph and how is chordal graph useful in solving semidefinite programs."
    />
    <meta name="keywords" lang="en" content="Chordal Graph, Semidefinite Programs, Semidefinite Programing." />
    <link rel="preconnect" href="https://fonts.gstatic.com" />
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet" />
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono&display=swap" rel="stylesheet" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=STIX+Two+Text:ital,wght@0,400;0,500;0,700;1,400&display=swap" rel="stylesheet" />
    <link href="../../css/fontawesome.css" rel="stylesheet" />
    <link href="../../css/brands.css" rel="stylesheet" />
    <link href="../../css/solid.css" rel="stylesheet" />
    <link href="../../css/regular.css" rel="stylesheet" />
    <link rel="icon" type="image/png" sizes="152x152" href="../../favicon/favicon152.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="../../favicon/favicon152.png" />
    <link rel="stylesheet" href="../../css/main.css" />
    <link rel="stylesheet" href="../../css/posts.css" />
    <link rel="stylesheet" href="../../css/topnav.css" />
    <link rel="stylesheet" href="../../css/footer.css" />
    <link rel="stylesheet" href="../../css/latex.css" />
    <script src="../../js/myscript.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
        },
      };
    </script>
    <style>
      :target {
        padding-top: 0px;
        margin-top: 0px;
      }
      .topnav {
        position: relative;
      }
      .clink {
        font-weight: 400;
        color: #1e3877;
        cursor: pointer;
      }
      p {
        margin-bottom: 5px;
      }
      body {
        text-align: justify;
      }
      @media screen and (max-width: 600px) {
        body {
          text-align: left;
        }
      }
      .MathJax {
        font-size: 1em !important;
      }
    </style>
  </head>

  <body>
    <header class="topnav">
      <a id="firstnav" href="https://hong-ming.github.io/">Hong-Ming Chiu</a>
      <div class="dropdown">
        <button class="dropbtn" onclick="topnavFunction()"><i class="fas fa-bars"></i></button>
        <div class="dropdown-content" id="myDropdown">
          <a href="/">Home</a>
          <a href="../../index.html#mybio">Biography</a>
          <a href="../../index.html#myexp">Experiences</a>
          <a href="../../index.html#mypublications">Publications</a>
          <a href="../../index.html#myprojects">Projects</a>
          <a href="/posts/" id="lastnav">Posts</a>
        </div>
      </div>
    </header>

    <main class="page-content">
      <div class="post-wrapper">
        <div class="page-title">Chordal Graph and Semidefinite Programs</div>
        <div class="link-path">
          <i class="fas fa-home"></i> <a href="/">Home</a>/<a href="/posts/">Posts</a>/<span style="text-decoration: underline">Chordal Graph and Semidefinite Programs</span>
        </div>

        <div class="post-content">
          <p>
            This post is about chordal graph and semidefinite programs (SDP), one of the research topic I found very interesting. The purpose of this post is to help people understand the basic
            concept of chordal graph and how is chordal graph useful in solving semidefinite programs. This post is divided into three main topics:
          </p>
          <ul class="fa-ul" style="text-align: left">
            <li>
              <span class="fa-li" style="top: -0.2em"><i class="fas fa-circle" style="font-size: 0.4em"></i></span>
              <a class="clink" onclick="SmoothScrollToAnchor('introduction')">What are the fundamental difficulties of solving SDP?</a>
            </li>
            <li>
              <span class="fa-li" style="top: -0.2em"><i class="fas fa-circle" style="font-size: 0.4em"></i></span>
              <a class="clink" onclick="SmoothScrollToAnchor('chordal-graph')">What is chordal graph and chordal sturcture?</a>
            </li>
            <li>
              <span class="fa-li" style="top: -0.2em"><i class="fas fa-circle" style="font-size: 0.4em"></i></span>
              <a class="clink" onclick="SmoothScrollToAnchor('chordal-graph-and-sdp')">How to exploit chordal structure in SDP?</a>
            </li>
          </ul>
          <div id="introduction" class="section" text="The Fundamental Difficulties of Solving SDP">
            <div class="subsection" text="Denseness of SDP">
              <p>
                Given a standard form semidefinite program (SDP) and its dual
                <img id="sdp" src="equ0.png" class="equpic" />
                where ${\bf A}_i,\ {\bf C},\ {\bf X},\ {\bf S}\in\mathrm{S^n}$, ${\bf b},\ {\bf y}\in\mathbb R^n$ and ${\bf C}\bullet{\bf X}=\mathrm{Tr}({\bf C}{\bf X})$. The fundamental difficulty of
                solving SDP is that the variable ${\bf X}$ is generally dense, even in the case when ${\bf C}$ and ${\bf A}_i$'s are sparse. It might seem straightforward to exploit the sparsity
                pattern in the dual problem, where the slack variable ${\bf S}$ has the (aggregated) sparsity pattern of ${\bf C},\ {\bf A}_1,\ldots ,{\bf A}_m$. Nonetheless, compute the gradient of
                log-barrier function $\phi({\bf S})=\log\det{{\bf S}}$ requires the inverse of ${\bf S}$, which is generally a dense matrix.
              </p>
              <div class="definition" text="Sparsity Pattern">
                A sparsity pattern $\mathrm{E}=\{(i,j)\}$ is a set of position, and a matrix ${\bf A}$ is said to have the sparsity pattern of $\mathrm{V}$ if for all $(i,j)\notin\mathrm{E}$ we have
                $A_{i,j}=0$. It is assume that all the diagonal entires are in $\mathrm{E}$.
              </div>
            </div>
            <div class="subsection" text="Per-iteration Cost of Interior-Point Method is High">
              <p>Although SDP can be efficiently solved by interior-point method (IPM) in just a few iterations, however, at each iteration of IPM, we have to solve a normal equation</p>
              <img src="equ28.png" class="equpic" />
              <p>
                to evaluate the Newton step $\Delta{\bf y}$. In general, ${\bf H}\in\mathrm{S^m}$ is fully dense; therefore $O(m^3)$ time is required to form ${\bf H}$ and solve $\Delta{\bf y}$, this
                result in $O(n^6)$ time complexity per-iteration because $n\leq m^2$ and $O(n^{6.5}\log(1/\epsilon))$ time to obtain the $\epsilon$-accuracy solution using IPM.
              </p>
              <p>
                When the SDP is chordal sparse. That is, the (aggregated) sparsity pattern of ${\bf C},{\bf A}_1,\ldots,{\bf A}_m$ is chordal sparse. We can achieve lower computational complexity to
                form ${\bf H}$ and solve $\Delta{\bf y}$ at each iteration of IPM. In this blog post, we elaborate a method to exploit chordal structure in SDP proposed by Zhang et al.
                <a class="cite" href="https://link.springer.com/article/10.1007/s10107-020-01516-y" target="_blank">1</a>.
              </p>
              <p>
                <span style="font-weight: 700">Notation: </span> Uppercase (lowercase) bold face letters indicate matrices (column vectors). $A_{ij}$ denotes the $(i,j)$-th element of ${\bf A}$. ${\rm
                Tr}(\cdot)$ denotes the trace of the matrix. $\mathrm{G(V,E)}$ denotes a graph with vertex set $\mathrm V$ and edge set $\mathrm E\subseteq \mathrm{V\times V}$. $\mathrm{S}^{n}$ is the
                set of symmetric matrices of order $n$. $\mathrm{S_+^n}$ and $\mathrm{S_{++}^n}$ are the sets of positive semidefinite, positive definite matrices, respectively. $\mathrm{S_E^n}$ is a
                set of symmetric matrices that have sparsity pattern $\mathrm E$. $\mathrm{S_{E,+}^{n}}$ and $\mathrm{S_{E,++}^{n}}$ are the sets of positive semidefinite and positive definite
                matrices in $\mathrm{S_E^n}$.
              </p>
            </div>
          </div>
          <div id="chordal-graph" class="section" text="Chordal Graph">
            <p>In many applications, we can break the underlying problems into solving a series of sparse normal equations</p>
            <img src="equ27.png" class="equpic" />
            <p>
              where ${\bf A}$ is sparse and ${\bf A}\succ 0$. A standard approach to solve ${\bf x}$ is to perform Cholesky factorization on ${\bf A}={\bf L}{\bf L}^\mathrm{T}$, and then solve ${\bf
              L}{\bf d}={\bf b}$ following by ${\bf L}^\mathrm{T}{\bf x}={\bf d}$. Since ${\bf L}$ is a lower triangular matrix, solving ${\bf L}{\bf d}={\bf b}$ and ${\bf L}^\mathrm{T}{\bf x}={\bf
              d}$ only requires back substitution, the computational complexity to solve ${\bf x}$ is cubic time if ${\bf L}$ is dense, but achieve linear time if ${\bf L}$ is sparse. In this section,
              we will show that if ${\bf A}$ has chordal structure, then its sparsity structure remains the same after Cholesky factorization.
            </p>
            <div class="definition" text="Chordal Graph">An undirected graph is chordal if every induce cycle has length 3.</div>
            <div class="subsection" text="Vertex Elimination">
              <p>To understand chordal structure, one must learn the concept of vertex elimination. Given an undirected graph $\mathrm{G(V,E)}$, the vertex elimination is defined as follow.</p>
              <ol style="margin-bottom: 5px">
                <li>Pick any vertex $v\in\mathrm V$ and remove it from the graph.</li>
                <li>Connect all the neighbors of $v$ together.</li>
                <li>Repeat step 1 and 2 until $\mathrm V=\{\emptyset\}$.</li>
              </ol>
              <p>The number of edges introduced during the process of vertex elimination depends on the order of elimination.</p>
              <div class="example" id="exp1">
                Consider a star graph
                <img src="fig0.png" class="equpic" />
                <p><span style="font-weight: 700">Case 1: </span> elimination order $=(v_1\to v_2\to v_3\to v_4\to v_5)$</p>
                <img src="fig1.png" class="equpic" />
                <p><span style="font-weight: 700">Case 2: </span> elimination order $=(v_2\to v_3\to v_4\to v_5\to v_1)$</p>
                <img src="fig2.png" class="equpic" />
                <p>
                  In case 1, we add 6 edges during the process of elimination. However, in case 2, we do not have to add any edges. We call the elimination order that does not introduce new edges
                  during the vertex elimination the perfect elimination ordering.
                </p>
              </div>
              <div class="definition" , text="Perfect Elimination Ordering">
                An elimination order is call the perfect elimination ordering if there is no edge being added during the process of vertex elimination.
              </div>
            </div>
            <div class="subsection" text="Cholesky Factorization">
              <p>
                he idea of vertex elimination is tightly related to Cholesky factorization. In fact, if the graphical structure of ${\bf A}$ has the perfect elimination ordering, the Cholesky factor
                of ${\bf A}$ will have the same structure if rows and columns of ${\bf A}$ are arranged in perfect elimination order. This idea is illustrated as follow.
              </p>
              <div class="example">
                Using the same graph and elimination order in Example <a class="ref" href="#exp1"> 1</a>, and let the number in each vertex denotes the order of elimination.
                <p><span style="font-weight: 700">Case 1: </span> rows and columns of ${\bf A}$ are not in perfect elimination ordering</p>
                <img src="fig3.png" class="equpic" />
                <p><span style="font-weight: 700">Case 2: </span> rows and columns of ${\bf A}$ are in perfect elimination ordering</p>
                <img src="fig4.png" class="equpic" />
              </div>
              <p>
                The element in matrix that were zeros in ${\bf A}$ but become non-zero in ${\bf L}$ is called fill-in (the elements marked in red in case 1). In case 1, the Cholesky factor has 6
                fill-in and become fully dense. However, if we permute the ${\bf A}$ into the perfect elimination order as shown in case 2, the Cholesky factor has no fill-in. Here, we give a
                conceptual explanation for this result, for more detailed and rigorous proof, see Appendix <a class="ref" href="#appA">A</a>. Partition ${\bf A}$ and compute its Schur complement, we
                have
              </p>
              <img src="equ1.png" class="equpic" />
              <p>
                This expression gives us the first column of Cholesky factor $\sqrt{d_1}\begin{bmatrix}1\\\frac{1}{d_1}{\bf b}_1\end{bmatrix}$. Obviously, the fill-in are cause by the term ${\bf
                b}_1{\bf b}_1^\mathrm{T}$ in ${\bf D}_1$. Let ${\bf a}_1=\begin{bmatrix}d_1\\{\bf b}_1\end{bmatrix}$ be the first column of ${\bf A}$. Because of the term ${\bf b}_1{\bf
                b}_1^\mathrm{T}$ in ${\bf D}_1$, if $(1,i)$-th and $(1,j)$-th elements in ${\bf a}_1$ is not zero, $i,j>1$, then the $(i,j)$-th element of ${\bf D}_1$ becomes nonzero. Graphically, it
                is equivalent to saying that if $(1,i)$, $(1,j)$ are connected, then $(i,j)$ will become connected after eliminating vertex 1, which is exactly the process of vertex elimination. The
                Cholesky factor can be obtained by performing the similar partition on ${\bf D}_1$ then compute its Schur complement to obtain ${\bf D}_2$, and then repeat this process until we get
                ${\bf D}_n$ (see Appendix <a class="ref" href="#appA">A</a> for details). If rows and columns of ${\bf A}$ are in perfect elimination order, then each ${\bf D}_1,\ldots,{\bf D}_n$ will
                have no fill-in, it follows that the Cholesky factor of ${\bf A}$ will have no fill-in as well.
              </p>
              <div class="lemma" text="no fill-in">
                Let $\boldsymbol{A}\succ 0$ be a symmetric matrix. If $\boldsymbol{A}$ is in perfect elimination ordering, then its Cholesky factor $\boldsymbol{L}$ has no fill-in.
              </div>
              <p>In the following, we use the matrix ${\bf A}$ in the case 1 above to illustrate this idea. Partition ${\bf A}$ and compute its Schur complement, we have</p>
              <img src="equ2.png" class="equpic" />
              <p>Observe that the graphical structure of ${\bf C}_1-\frac{1}{d_1}{\bf b}_1{\bf b}_1^\mathrm{T}$ is the graphical structure of ${\bf A}$ after eliminating vertex $1$.</p>
              <img src="fig5.png" class="equpic" />
              <p>
                Notice that not every graph has perfect elimination ordering, and the perfect elimination ordering (if exists) is not unique in general. Fulkerson et at.
                <a class="cite" href="https://msp.org/pjm/1965/15-3/pjm-v15-n3-p11-s.pdf" target="_blank">2</a> showed that graphs that have the perfect elimination ordering are exactly the chordal
                graphs.
              </p>
              <div class="theorem" text="Fulkerson et at. 1965">An undirected graph is chordal if and only if it has the perfect elimination ordering.</div>
            </div>
            <div class="subsection" text="Clique Tree">
              <p>
                Any chordal graph $\mathrm{G(V,E)}$ can be turn into a tree that has the clique (maximal complete subgraph) of $\mathrm G$ being its vertex. This idea is illustrated in the following
                example borrowed from <a class="cite" href="https://www.seas.ucla.edu/~vandenbe/publications/chordalsdp.pdf" target="_blank">3</a>.
              </p>
              <div class="example" id="exp3">
                The chordal graph in p.29 of <a class="cite" href="https://www.seas.ucla.edu/~vandenbe/publications/chordalsdp.pdf" target="_blank">3</a> and its clique tree. Let $W$ denotes a set of
                cliques.
                <img src="fig6.png" class="equpic" />
              </div>
            </div>
            <div class="subsection" text="Induce Subtree, Running Intersection Property">
              <p>
                In Example <a class="ref" href="#exp3">3</a>, observe that for any $v\in\mathrm V$, every vertex in the clique tree contains $v$ induce a subtree. Therefore, if $v$ is contained in two
                cliques $W_1$ and $W_2$, then $v$ has to be contained in every cliques that are in the path from $W_1$ to $W_2$ on the clique tree. This is known as induce subtree or running
                intersection property.
              </p>
              <div class="example">
                The induce subtree of vertex $e$ and $c$.
                <img src="fig7.png" class="equpic" />
              </div>
            </div>
          </div>
          <div id="chordal-graph-and-sdp" class="section" text="Exploit Chordal Structure in SDP">
            <div class="subsection" text="Clique Tree Conversion for SDP">
              <p>
                Suppose ${\bf C}, {\bf A}_1,\ldots,{\bf A}_m\in \mathrm{S_E^n}$.
                <a class="cite" href="https://epubs.siam.org/doi/abs/10.1137/S105262340240851X?mobileUi=0&" target="_blank">4</a>
                <a class="cite" href="https://link.springer.com/article/10.1007/s10107-002-0351-9" target="_blank">5</a>
                <a class="cite" href="https://link.springer.com/article/10.1007/s10107-002-0351-9" target="_blank">6</a>
                <a class="cite" href="https://arxiv.org/abs/cs/0412009" target="_blank">7</a>
                proposed to reformulated SDP as
              </p>
              <img src="equ3.png" class="equpic" />
              <p>where $\mathrm{P_E(S_+^n)}=\left\{\mathrm{P_E}({\bf X}) \mid {\bf X} \succeq 0\right\}$ is the cone of matrices in $\mathrm{S_E^n}$ that have a positive semidefinite completion.</p>
              <div class="definition" text="Projection onto Sparsity Pattern">
                A projection ${\bf Y}$ of a matrix ${\bf X}$ onto a sparsity pattern is denoted ${\bf Y}=\mathrm{P_E({\bf X})}$, i.e., $Y_{i,j}=X_{i,j}$ if $(i,j)\in\mathrm E$ and otherwise
                $Y_{i,j}=0$.
              </div>
              <p>
                If we further suppose $\mathrm{G(V,E)}$ is chordal. Let $W_1,\ldots,W_l$ denotes the cliques of $\mathrm G$, and ${\bf X}[W_j,W_j]$ denotes sub-matrices corresponding to clique $W_j$.
                Grone et al. <a class="cite" href="https://www.seas.ucla.edu/~vandenbe/publications/chordalsdp.pdf" target="_blank">7</a> showed that this problem can be further relaxed into the
                optimization problem that only enforce the positive semidefiniteness constraint on each ${\bf X}[W_j,W_j]$
              </p>
              <img src="equ4.png" class="equpic" />
              <div class="theorem" text="Grone et al. 1984" id="thm2">
                $\boldsymbol{X}$ is positive semidefinite completable, or $\boldsymbol{X}\in P_E(S_+^n)$, if and only if $\boldsymbol{X}[W_j,W_j]\succeq 0$ for all $j=1,\ldots,l$.
              </div>
              <p>
                By Theorem <a class="ref" href="#thm2">2</a>, the relaxation above is always possible, then it follows that the relaxation is tight because ${\bf C}\bullet{\bf X}={\bf C}\bullet
                \mathrm{P_E({\bf X})}$. For simplicity, let ${\bf X}_j={\bf X}[W_j,W_j]$. For some ${\bf C}_j,{\bf A}_{ij}\in\mathbb R^{|W_j|\times |W_j|}$, we can further rewrite this problem into
                the optimization problem over ${\bf X}_j$'s.
              </p>
              <img id="ctc" src="equ5.png" class="equpic" />
              <p>
                where $\mathcal N_{ij}(X_i)$ denotes the part of ${\bf X}_i$ overlaps with ${\bf X}_j$. The reformulation above is called the clique tree conversion (CTC) for SDP. It is worth noting
                that we only have to impose the overlaping constraint $\mathcal N_{ij}(X_i)=\mathcal N_{ji}(X_j)$ along the edge of clique tree because of the induce subtree properties. Example
                <a class="ref" href="#exp5">5</a> gives a concrete example for this reformulation.
              </p>
              <div class="example" id="exp5">
                Let the graph $\mathrm{G(V,E)}$ shown in the left-hand side be the graphical structure of ${\bf C},{\bf A}_1,\ldots,{\bf A}_m$. Notice that $\mathrm G$ is chordal and has two cliques
                $W_1$ and $W_2$. The corresponding ${\bf X}_1={\bf X}[W_1,W_1]$, ${\bf X}_2={\bf X}[W_2,W_2]$ and $\mathcal N_{1,2}({\bf X}_1)=\mathcal N_{2,1}({\bf X}_2)$ are shown in the right-hand
                side.
                <img src="fig8.png" class="equpic" />
              </div>
              <p>
                Notice that the ${\bf X}$ obtained from <a class="eqref" href="#ctc">2</a> does not guarantee to be positive semidefinite; therefore, when ${\bf X}$ is not positive semidefinite, we
                need to project ${\bf X}$ back to the positive semidefinite cone. This process is called positive semidefinite completion and is given as follow
              </p>
              <img id="psd-completion" src="equ6.png" class="equpic" />
              <p>
                Figure <a class="ref" href="#fig1">1</a> illustrate the process of solving SDP through the clique tree conversion <a class="eqref" href="#ctc">2</a> following by the positive
                semidefinite completion <a class="eqref" href="#psd-completion">3</a>
              </p>
              <img id="fig1" src="fig9.png" class="equpic" />
              <div class="caption" text="Figure 1">
                The graphical illustration of solving SDP <a class="eqref" href="#sdp">1</a> through clique tree conversion <a class="eqref" href="#ctc">2</a> and positive semidefinite completion
                <a class="eqref" href="#psd-completion">3</a>. The feasible set of <a class="eqref" href="#sdp">1</a> and <a class="eqref" href="#ctc">2</a> are shown in blue and orange respectively.
                $\widehat {\bf X}$ denotes an optimizer of <a class="eqref" href="#ctc">2</a>, and $\widehat {\bf Z}$ denotes the solution obtained after <a class="eqref" href="#psd-completion">3</a>,
                which is an optimizer of <a class="eqref" href="#sdp">1</a>. Obviously, if $\widehat{\bf X}$ were in blue set, then we can skip <a class="eqref" href="#psd-completion">3</a>.
              </div>
              <p>The clique tree conversion can be vectorizes into the standard form of linear conic program</p>
              <img id="vctc" src="equ7.png" class="equpic" />
              <p>
                where $\mathcal K=\mathrm{S^{|W_1|}_+}\times,\ldots,\times, \mathrm{S_+}^{|\mathrm W_l|}$. Let $f(n)=n(n+1)/2$ be a function outputs the number of elements in the lower triangular part
                of $n\times n$ matrix. $\text{svec}({\bf A})\in\mathbb R^{f(n)}$ be the weighted vectorization of the lower triangular part of $n\times n$ matrix ${\bf A}$ such that $\text{svec}({\bf
                A})^\mathrm{T}\text{svec}({\bf B})={\bf A}\bullet{\bf B}$. Then ${\bf c},{\bf x}\in\mathbb R^{n'}$, ${\bf A}\in\mathbb R^{m\times n'}$, $n'=\sum_{j=1}^lf(|W_j|)$ is given by
              </p>
              <img src="equ8.png" class="equpic" />
              <p>
                In addition ${\bf N}\in\mathbb R^{m'\times n'}$ is a block matrix, where $m'=\sum_{i=1}^lf(|W_i\cap W_{p(i)}|)$ and $p(i)$ is the parent of vertex $i$ in the clique tree. ${\bf N}$ and
                its blocks ${\bf N}_{i,j}\in\mathbb R^{f(|W_i\cap W_{p(i)}|)\times f(|W_j|)}$ are defined as
              </p>
              <img src="equ9.png" class="equpic" />
            </div>
            <div class="subsection" text="Partial Separability">
              <p>The constraint $\sum_{j=1}^l{\bf A}_{ij}\bullet{\bf X}_j=b_i$ is said to be partially separable if and only if there exists some ${\bf A}_i'$ and $i'\in\{1,\ldots,l\}$ such that</p>
              <img src="equ10.png" class="equpic" />
              <div class="example" text="MAX k-CUT">
                Frieze et at. <a class="cite" href="https://link.springer.com/article/10.1007/BF02523688" target="_blank">8</a> proposed a randomized algorithm to approximate MAX k-CUP base on solving
                <img src="equ11.png" class="equpic" />
                Observe that each constraint affect a single matrix element, therefore this problem is partially separable.
              </div>
              <p>When SDP is partially separable, the blocks of matrix ${\bf A}$ in <a class="eqref" href="#vctc">4</a> becomes</p>
              <img src="equ12.png" class="equpic" />
            </div>
            <div class="subsection" text="Dualize Clique Tree Conversion">
              <p>
                Zhang et al. <a class="cite" href="https://link.springer.com/article/10.1007/s10107-020-01516-y" target="_blank">1</a> proposed to exploit sparsity structure of ${\bf H}$ using the
                dualizing technique of Lofberg <a class="cite" href="https://www.tandfonline.com/doi/abs/10.1080/10556780802553325?journalCode=goms20" target="_blank">9</a>. The dual of
                <a class="eqref" href="#vctc">4</a> is given by
              </p>
              <img id="normal" src="equ13.png" class="equpic" />
              <p>
                where $\mathcal K^*=\mathcal K$ is the dual cone of $\mathcal K$. The dualization technique of Lofberg
                <a class="cite" href="https://www.tandfonline.com/doi/abs/10.1080/10556780802553325?journalCode=goms20" target="_blank">9</a> swap the role played by primal and dual problem
              </p>
              <img id="lofberg" src="equ14.png" class="equpic" />
              <p>
                where $f=m+m'$ is the number of equality constraint. Observe that the dual problem of <a class="eqref" href="#lofberg">6</a> is exactly the same as the primal problem in
                <a class="eqref" href="#normal">5</a>. Applying the IPM to solve <a class="eqref" href="#lofberg">6</a>. The normal equation we need to solve at each iteration takes the form
              </p>
              <img src="equ15.png" class="equpic" />
              <p>
                where ${\bf D}_s$ is a block diagonal matrix such that ${\bf D}_s=\text{diag}({\bf D}_{s1},\ldots,{\bf D}_{sl})$, ${\bf D}_{si}\in\mathbb R^{f(|W_i|)\times f(|W_i|)}$ and ${\bf
                D}_{s,i}\succeq 0$. ${\bf D}_f$ is low-rank perturbation such that ${\bf D}_f=\sigma{\bf I}+{\bf w}{\bf w}^\mathrm{T}$. Let
              </p>
              <img src="equ16.png" class="equpic" />
              <p>and apply Sherman-Morrison-Woodbury, we have</p>
              <img id="normal-eq" src="equ17.png" class="equpic" />
              <p>
                The block sparsity pattern of ${\bf N}^\mathrm{T}{\bf N}$ coincides with the tree graph $\mathrm{G(W,T)}$, and ${\bf A}^\mathrm{T}{\bf A}$ is a block diagonal matrix under the
                partially separable assumption. As a result, the block sparsity pattern of ${\bf H}$ is chordal. Example <a class="ref" href="#exp7">1</a> gives a concrete example.
              </p>
              <div class="example" id="exp7">
                Using the chordal graph $\mathrm{G(V,E)}$ and its tree graph $\mathrm{G(W,T)}$ in Example <a class="ref" href="#exp3">3</a> $(n=9,l=6)$. Suppose $m=5$ and $\{i'\}=\{2,4,1,2,5\}$, then
                the ${\bf A}$ and ${\bf A}^\mathrm{T}{\bf A}$ has the following structure
                <img src="equ18.png" class="equpic" />
                where ${\bf a}_i'=\text{svec}({\bf A}'_i)$, and $\tilde{\bf A}_i\in\mathbb R^{f(|W_i|)\times f(|W_i|)}$ are some dense matrix. Suppose we index the tree graph $\mathrm{G(W,T)}$ in
                post-order, then ${\bf N}$ and ${\bf N}^\mathrm{T}{\bf N}$ has the following structure
                <img src="equ19.png" class="equpic" />
                where $\tilde{\bf N}_{i,j}\in\mathbb R^{f(|W_i|)\times f(|W_j|)}$ are some dense matrix. Therefore, ${\bf D}_s+{\bf A}^\mathrm{T}{\bf A}+{\bf N}^\mathrm{T}{\bf N}$ has the following
                block structure
                <img src="equ20.png" class="equpic" />
                coincides with the graphical structure of tree graph $\mathrm{G(W,T)}$.
              </div>
              <div class="lemma" text="no block fill" id="lem2">
                Let $\boldsymbol{H}\succ 0$ be a symmetric block matrix. If the block sparsity pattern of $\boldsymbol{H}$ is chordal, then its Cholseky factor has no block fill.
              </div>
              <div class="proof">
                Adapt the proof in Appendix <a class="ref" href="#appA">A</a>. Partition ${\bf H}$ and compute its Schur complement, we have
                <img src="equ21.png" class="equpic" />
                The block fill-in is caused by ${\bf B}_1{\bf D}_{11}^{-1}{\bf B}_1^\mathrm{T}$ in ${\bf D}_1$. Then it follows that when the block sparsity pattern of ${\bf H}$ is chordal, then the
                block sparsity pattern of ${\bf C}_1$ will be the same as the block sparsity pattern of ${\bf B}_1{\bf D}_{11}^{-1}{\bf B}_1^\mathrm{T}$. The Cholesky factor can be obtained by
                applying the similar partition on ${\bf D}_1$ and computes its Schur complement, and then repeat this process until we obtain the block diagonal matrix ${\bf D}_n$.
              </div>
              <div class="qed">$\square$</div>
              <p>
                Observe that evaluating ${\bf H}^{-1}{\bf q}$ is the same as solving ${\bf q}'$ such that ${\bf H}{\bf q}'={\bf q}$. ${\bf q}'$ can be solved by first factorizing ${\bf H}={\bf L}{\bf
                L}^\mathrm{T}$, and then solve ${\bf L}{\bf b}={\bf q}$ following by ${\bf L}^\mathrm{T}{\bf q}'={\bf b}$. By Lemma <a class="ref" href="#lem2">2</a>, ${\bf L}$ has no block fill;
                therefore, ${\bf L}$ can be factorized in $O(\beta^3n)$ and ${\bf q}'$ can be solved in $O(\beta^2n)$, where $\beta=\max_jf(|W_j|)$. This result allows us to evaluate $\Delta{\bf y}$
                in <a class="eqref" href="#normal-eq">7</a> in $O(\omega^6n)$ time, where $\omega=\max_j|W_j|$ and $\beta\leq\omega^2$. As a result, the computational complexity to obtain the
                $\epsilon$-accuracy solution using IPM is $O(w^6n^{1.5}\log(1/\epsilon))$. Notice that Zhang's method
                <a class="cite" href="https://link.springer.com/article/10.1007/s10107-020-01516-y" target="_blank">1</a> is better than Andersen's method
                <a class="cite" href="https://link.springer.com/article/10.1007/s12532-010-0016-2" target="_blank">10</a> when the number of equality constraints $m$ is large. Specifically, when $m$
                is of $O(\omega n)$ then Andersen's method requires $O(\omega^3n^{3.5}\log(1/\epsilon))$ time complexity, which is comparable to the cubic time complexity of direct IPM when the number
                of constraint $m$ is of $O(n)$.
              </p>
            </div>
          </div>
          <div id="appendix" class="section" text="Appendix">
            <div id="appA" class="appsubsection" text="Proof for Lemma 1">
              <div class="proof">
                Suppose ${\bf A}\in\mathrm{S_{E}^n}$. Partition ${\bf A}$ and computes its Schur complement, we have
                <img src="equ1.png" class="equpic" />
                We can easily see that ${\bf L}_1+{\bf L}_1^\mathrm T\in\mathrm{S_{E}^n}$. Let $\mathrm{E_1}$ be the sparsity pattern of ${\bf D}_1$, then $\mathrm{E_1}$ is simply the union of
                sparsity pattern $\mathrm{E}$ and any new edges introduced by the outer product of ${\bf b}_1$
                <img src="equ22.png" class="equpic" />
                Because ${\bf A}$ is in perfect elimination ordering we have $\{(i,j)\mid (1,i)\in\mathrm E,\ (1,j)\in\mathrm E\}\subseteq\mathrm E$, therefore $\mathrm E_1=\mathrm E$ and ${\bf
                D}_1\in\mathrm{S_{E}^n}$. Similarly, we can apply the same partition on ${\bf D}_1$ and factor it as ${\bf D}_1=\widetilde{\bf L}_2{\bf D}_2\widetilde{\bf L}^\mathrm{T}_2$ where
                <img src="equ23.png" class="equpic" />
                and let ${\bf L}_2={\bf L}_1\widetilde{\bf L}_2$, we have
                <img src="equ24.png" class="equpic" />
                Let $\mathrm{E_2}$ be the sparsity pattern of ${\bf D}_2$, becasue ${\bf A}$ is in perfect elimination ordering we have
                <img src="equ25.png" class="equpic" />
                Repeating this procedure $n$ times, we have ${\bf A}={\bf L}_n{\bf D}_n{\bf L}_n^\mathrm T$, where
                <img src="equ26.png" class="equpic" />
                Because ${\bf A}$ is in perfect elimination ordering we have $\mathrm E_n =\mathrm E_{n-1} = \ldots = \mathrm E_1= \mathrm E$. Since ${\bf D}_n$ is diagonal and ${\bf L}={\bf L}_n{\bf D}_n^{\frac{1}{2}}$, we
                conclude that ${\bf L}+{\bf L}^\mathrm T\in\mathrm{S^n_E}$; therefore, the Cholesky factor ${\bf L}$ has no fill-in.
              </div>
              <div class="qed">$\square$</div>
            </div>
          </div>
          <div id="reference" class="section" text="Reference">
            <div class="bib">
              <a class="bib-left" href="https://link.springer.com/article/10.1007/s10107-020-01516-y" target="_blank">1</a>
              <span class="bib-right">
                Zhang, R.Y., Lavaei, J. "Sparse semidefinite programs with guaranteed near-linear time complexity via dualized clique tree conversion." Mathematical Programming 188, 351-393 (2021).
              </span>
            </div>
            <div class="bib">
              <a class="bib-left" href="https://msp.org/pjm/1965/15-3/pjm-v15-n3-p11-s.pdf" target="_blank">2</a>
              <span class="bib-right">D. R. Fulkerson and O. Gross. "Incidence matrices and interval graphs." Pacific Journal of Mathematics, 15(3):835-855, 1965.</span>
            </div>
            <div class="bib">
              <a class="bib-left" href="https://www.seas.ucla.edu/~vandenbe/publications/chordalsdp.pdf" target="_blank">3</a>
              <span class="bib-right">Lieven Vandenberghe and Martin S. Andersen, "Chordal Graphs and Semideﬁnite Optimization," 2015.</span>
            </div>
            <div class="bib">
              <a class="bib-left" href="https://epubs.siam.org/doi/abs/10.1137/S105262340240851X?mobileUi=0&" target="_blank">4</a>
              <span class="bib-right">Samuel Burer, "Semideﬁnite programming in the space of partial positive semideﬁnite matrices," SIAM J. Optim., vol. 14, pp. 139-172, 2003.</span>
            </div>
            <div class="bib">
              <a class="bib-left" href="https://link.springer.com/article/10.1007/s10107-002-0351-9" target="_blank">5</a>
              <span class="bib-right">
                Mituhiro Fukuda, Masakazu Kojima, Kazuo Murota, and Kazuhide Nakata, "Exploiting sparsity in semideﬁnite programming viamatrix completion i: General framework," vol. 11, no. 3, 2000.
              </span>
            </div>
            <div class="bib">
              <a class="bib-left" href="https://link.springer.com/article/10.1007/s10107-002-0351-9" target="_blank">6</a>
              <span class="bib-right">
                Mituhiro Fukuda, Masakazu Kojima, Kazuhide Nakata, Katsuki Fujisawa and Kazuo Murota, "Exploiting sparsity in semideﬁnite programming viamatrix completion ii: implementation and
                numerical results," 2003.
              </span>
            </div>
            <div class="bib">
              <a class="bib-left" href="https://arxiv.org/abs/cs/0412009" target="_blank">7</a>
              <span class="bib-right">
                Gun Srijuntongsiri and Stephen A. Vavasis, "A fully sparse implementation of a primal-dual interior-point potential reduction method for semideﬁnite programming," ArXiv, vol.
                abs/cs/0412009, 2004.
              </span>
            </div>
            <div class="bib">
              <a class="bib-left" href="https://link.springer.com/article/10.1007/BF02523688" target="_blank">8</a>
              <span class="bib-right">
                Alan Frieze and M. Jerrum, "Jerrum, m.: Improved approximation algorithms for max k-cut and max bisection. algorithmica 18(1), 67-81," Algorithmica, vol. 18, pp. 67-81, 05 1997.
              </span>
            </div>
            <div class="bib">
              <a class="bib-left" href="https://www.tandfonline.com/doi/abs/10.1080/10556780802553325?journalCode=goms20" target="_blank">9</a>
              <span class="bib-right">
                Johan Lofberg, "Dualize it: software for automatic primal and dual conversions of conic programs," OptimizationMethods andSoftware, vol. 24, no. 3, pp. 313-325, 2009.
              </span>
            </div>
            <div class="bib">
              <a class="bib-left" href="https://link.springer.com/article/10.1007/s12532-010-0016-2" target="_blank">10</a>
              <span class="bib-right">
                Martin Andersen, Joachim Dahl, and Lieven Vandenberghe, "Implementation of nonsym- metric interior-point methods for linear optimization over sparse matrix cones, Mathematical
                Programming Computation, vol. 2, pp. 167-201, 12 2010.
              </span>
            </div>
          </div>
        </div>
      </div>
    </main>

    <footer class="page-footer" style="margin-top: 50px">
      <div class="back-to-top-botton">
        <div class="back-to-top" onclick="SmoothScrollToTop()">Scroll to Top</div>
      </div>
      <div class="myfooter">
        <div class="foot-left">
          <a href="mailto: hongmingchiu0217@gmail.com" class="ficon-button femail"> <i class="fas fa-envelope icon-email"></i><span></span><span id="foot-icon-text">Gmail</span></a>

          <a href="https://www.facebook.com/hmchiu2/" class="ficon-button ffacebook"> <i class="fab fa-facebook-f icon-facebook"></i><span></span><span id="foot-icon-text">Facebook</span></a>

          <a href="https://www.linkedin.com/in/hmchiu/" class="ficon-button flinkedin"> <i class="fab fa-linkedin-in icon-linkedin"></i><span></span><span id="foot-icon-text">LinkedIn</span></a>

          <a href="javascript:void(0)" class="ficon-button ftwitter"> <i class="fab fa-twitter icon-twitter"></i><span></span><span id="foot-icon-text">Twitter</span></a>

          <a href="https://www.instagram.com/_hongming/" class="ficon-button finstagram"> <i class="fab fa-instagram icon-instagram"></i><span></span><span id="foot-icon-text">Instagram</span></a>

          <a href="/sitemap/" class="ficon-button fsitemap"> <i class="fas fa-sitemap icon-sitemap"></i><span></span><span id="foot-icon-text">Sitemap</span></a>
        </div>
        <div class="foot-right">
          <p>
            This is an academic website for Hong-Ming Chiu to share his experiences, publications and projects. Some designs of this website are borrowed and modified from
            <a href="http://hexianghu.com/">Hexiang (Frank) Hu</a>'s.
          </p>
        </div>
      </div>
    </footer>
  </body>
</html>
